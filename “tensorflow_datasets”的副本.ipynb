{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“tensorflow/datasets”的副本",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JAMES-YI/00_Tensorflow_Tutorials/blob/master/%E2%80%9Ctensorflow_datasets%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6XvCUmCEd4Dm"
      },
      "source": [
        "Codes from www.Tensorflow.org\n",
        "\n",
        "Modified by JYI, 04/13/2020\n",
        "\n",
        "- TFDS provides a collection of ready-to-use datasets. It handles downloading and preparing the data and constructing a [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n",
        "- Do not confuse [TFDS](https://www.tensorflow.org/datasets) (this library) with [tf.data](https://www.tensorflow.org/guide/data) (TensorFlow API to build efficient data pipelines). TFDS is a high level wrapper around `tf.data`.\n",
        "- Please include the following citation when using `tensorflow-datasets` for a paper, in addition to any citation specific to the used datasets.\n",
        "\n",
        "```\n",
        "@misc{TFDS,\n",
        "  title = { {TensorFlow Datasets}, A collection of ready-to-use datasets},\n",
        "  howpublished = {\\url{https://www.tensorflow.org/datasets}},\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J8y9ZkLXmAZc"
      },
      "source": [
        "Copyright 2018 The TensorFlow Datasets Authors, Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGw9EgE0tC0C"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/datasets/overview\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/datasets/blob/master/docs/overview.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xLuLaJIYGajd"
      },
      "source": [
        "## Eager execution\n",
        "\n",
        "- TensorFlow Datasets is compatible with both TensorFlow [Eager mode](https://www.tensorflow.org/guide/eager) and Graph mode\n",
        "- difference between eager mode and graph mode? \n",
        "- For this colab, we'll run in Eager mode, which is the default in TensorFlow 2.\n",
        "- Each dataset is implemented as a [`tfds.core.DatasetBuilder`](https://www.tensorflow.org/datasets/api_docs/python/tfds/core/DatasetBuilder) and you can list all available builders with `tfds.list_builders()`.\n",
        "- You can see all the datasets with additional documentation on the [datasets documentation page](https://www.tensorflow.org/datasets/catalog/overview)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RxCjcc7wGaPQ",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow tensorflow-datasets matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "tf.executing_eagerly()\n",
        "tfds.list_builders() # give all the built dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VjI6VgOBf0v0"
      },
      "source": [
        "## `tfds.load`: A dataset in one line\n",
        "\n",
        "- [`tfds.load`](https://www.tensorflow.org/datasets/api_docs/python/tfds/load) is a convenience method that's the simplest way to build and load a `tf.data.Dataset`.\n",
        "- `tf.data.Dataset` is the standard TensorFlow API to build input pipelines. If you're not familiar with this API, we **strongly** encourage you to read [the official TensorFlow guide](https://www.tensorflow.org/guide/datasets).\n",
        "- once data has been prepared, subsequent calls of `load` will reuse the prepared data.\n",
        "- You can customize where the data is saved/loaded by specifying `data_dir=` (\n",
        "defaults to `~/tensorflow_datasets/`).\n",
        "- [documentation on datasets versioning](https://github.com/tensorflow/datasets/blob/master/docs/) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dCou80mnLLPV",
        "colab": {}
      },
      "source": [
        "ds_train = tfds.load(name=\"mnist\", split=\"train\")\n",
        "assert isinstance(ds_train, tf.data.Dataset)\n",
        "print(ds_train)\n",
        "ds_all = tfds.load(\"mnist:3.*.*\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u-GAxR79hGTr"
      },
      "source": [
        "## Feature dictionaries\n",
        "\n",
        "- All `tfds` datasets contain feature dictionaries mapping feature names to Tensor values. A typical dataset, like MNIST, will have 2 keys: `\"image\"` and `\"label\"`. Below we inspect a single example.\n",
        "- how to access the values of samples\n",
        "- In graph mode, see the [tf.data guide](https://www.tensorflow.org/guide/datasets#creating_an_iterator) to understand how to iterate on a `tf.data.Dataset`.\n",
        "- define the rest of an input pipeline suitable for model training by using the [`tf.data` API](https://www.tensorflow.org/guide/datasets).\n",
        "- we'll repeat the dataset so that we have an infinite stream of examples, shuffle, and create batches of 32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YHE21nkHLrER",
        "colab": {}
      },
      "source": [
        "# access sample value\n",
        "for example in ds_train.take(1):  # Only take a single example\n",
        "  image, label = example[\"image\"], example[\"label\"]\n",
        "\n",
        "  plt.imshow(image.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap(\"gray\"))\n",
        "  print(\"Label: %d\" % label.numpy())\n",
        "\n",
        "# another way for loading dataset\n",
        "mnist_builder = tfds.builder(\"mnist\")\n",
        "mnist_builder.download_and_prepare()\n",
        "ds_train = mnist_builder.as_dataset(split=\"train\")\n",
        "ds_train\n",
        "\n",
        "# preprocessing for training\n",
        "ds_train = ds_train.repeat().shuffle(1024).batch(32)\n",
        "\n",
        "# prefetch will enable the input pipeline to asynchronously fetch batches while\n",
        "# your model is training.\n",
        "ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Now you could loop over batches of the dataset and train\n",
        "# for batch in ds_train:\n",
        "#   ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uczpNuc_A7wE"
      },
      "source": [
        "## DatasetInfo\n",
        "\n",
        "After generation, the builder contains useful information on the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mSamfFznA9Ph",
        "colab": {}
      },
      "source": [
        "info = mnist_builder.info\n",
        "print(info)\n",
        "\n",
        "print(info.features)\n",
        "print(info.features[\"label\"].num_classes)\n",
        "print(info.features[\"label\"].names)\n",
        "\n",
        "ds_test, info = tfds.load(\"mnist\", split=\"test\", with_info=True)\n",
        "print(info)\n",
        "\n",
        "fig = tfds.show_examples(info, ds_test) # sample demonstration"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}